{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Bi-LSTM model in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a simple Bi-LSTM model in tensorflow to demonestrate\n",
    "how bi-lstm model work\n",
    "\n",
    "## Steps:\n",
    "\n",
    "* Text Preprocessing\n",
    "* Building Model\n",
    "* Training \n",
    "* Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sagor Sarker\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing(Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example data\n",
    "\n",
    "sentences = (\"He is going to Dhaka\", \"She lives in Rangpur\", \"We love our country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'is', 'going', 'to', 'Dhaka', 'She', 'lives', 'in', 'Rangpur', 'We', 'love', 'our', 'country']\n",
      "['Rangpur', 'our', 'country', 'She', 'We', 'to', 'going', 'is', 'lives', 'love', 'Dhaka', 'He', 'in']\n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences\n",
    "word_seq = \" \".join(sentences).split()\n",
    "print(word_seq)\n",
    "# remove duplicate words\n",
    "word_list = list(set(word_list))\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rangpur': 0, 'our': 1, 'country': 2, 'She': 3, 'We': 4, 'to': 5, 'going': 6, 'is': 7, 'lives': 8, 'love': 9, 'Dhaka': 10, 'He': 11, 'in': 12}\n",
      "{0: 'Rangpur', 1: 'our', 2: 'country', 3: 'She', 4: 'We', 5: 'to', 6: 'going', 7: 'is', 8: 'lives', 9: 'love', 10: 'Dhaka', 11: 'He', 12: 'in'}\n"
     ]
    }
   ],
   "source": [
    "# create word2id\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "print(word_dict)\n",
    "# create id2word\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "print(number_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "n_class = len(word_dict)\n",
    "print(n_class)\n",
    "n_step = len(word_list)\n",
    "print(n_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input words: \n",
      "['He']\n",
      "[11]\n",
      "[11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "target word:  is\n",
      "7\n",
      "input words: \n",
      "['He', 'is']\n",
      "[11, 7]\n",
      "[11, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "target word:  going\n",
      "6\n",
      "input words: \n",
      "['He', 'is', 'going']\n",
      "[11, 7, 6]\n",
      "[11, 7, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "target word:  to\n",
      "5\n",
      "input words: \n",
      "['He', 'is', 'going', 'to']\n",
      "[11, 7, 6, 5]\n",
      "[11, 7, 6, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "target word:  Dhaka\n",
      "10\n",
      "input words: \n",
      "['He', 'is', 'going', 'to', 'Dhaka']\n",
      "[11, 7, 6, 5, 10]\n",
      "[11, 7, 6, 5, 10, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "target word:  She\n",
      "3\n",
      "input words: \n",
      "['He', 'is', 'going', 'to', 'Dhaka', 'She']\n",
      "[11, 7, 6, 5, 10, 3]\n",
      "[11, 7, 6, 5, 10, 3, 0, 0, 0, 0, 0, 0, 0]\n",
      "target word:  lives\n",
      "8\n",
      "input words: \n",
      "['He', 'is', 'going', 'to', 'Dhaka', 'She', 'lives']\n",
      "[11, 7, 6, 5, 10, 3, 8]\n",
      "[11, 7, 6, 5, 10, 3, 8, 0, 0, 0, 0, 0, 0]\n",
      "target word:  in\n",
      "12\n",
      "input words: \n",
      "['He', 'is', 'going', 'to', 'Dhaka', 'She', 'lives', 'in']\n",
      "[11, 7, 6, 5, 10, 3, 8, 12]\n",
      "[11, 7, 6, 5, 10, 3, 8, 12, 0, 0, 0, 0, 0]\n",
      "target word:  Rangpur\n",
      "0\n",
      "input words: \n",
      "['He', 'is', 'going', 'to', 'Dhaka', 'She', 'lives', 'in', 'Rangpur']\n",
      "[11, 7, 6, 5, 10, 3, 8, 12, 0]\n",
      "[11, 7, 6, 5, 10, 3, 8, 12, 0, 0, 0, 0, 0]\n",
      "target word:  We\n",
      "4\n",
      "input words: \n",
      "['He', 'is', 'going', 'to', 'Dhaka', 'She', 'lives', 'in', 'Rangpur', 'We']\n",
      "[11, 7, 6, 5, 10, 3, 8, 12, 0, 4]\n",
      "[11, 7, 6, 5, 10, 3, 8, 12, 0, 4, 0, 0, 0]\n",
      "target word:  love\n",
      "9\n",
      "input words: \n",
      "['He', 'is', 'going', 'to', 'Dhaka', 'She', 'lives', 'in', 'Rangpur', 'We', 'love']\n",
      "[11, 7, 6, 5, 10, 3, 8, 12, 0, 4, 9]\n",
      "[11, 7, 6, 5, 10, 3, 8, 12, 0, 4, 9, 0, 0]\n",
      "target word:  our\n",
      "1\n",
      "input words: \n",
      "['He', 'is', 'going', 'to', 'Dhaka', 'She', 'lives', 'in', 'Rangpur', 'We', 'love', 'our']\n",
      "[11, 7, 6, 5, 10, 3, 8, 12, 0, 4, 9, 1]\n",
      "[11, 7, 6, 5, 10, 3, 8, 12, 0, 4, 9, 1, 0]\n",
      "target word:  country\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# make batch input for lstm from sentence data\n",
    "def make_batch_debug(sentence):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    words = word_seq\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        print('input words: ')\n",
    "        print(words[:(i+1)])\n",
    "        input = [word_dict[n] for n in words[:(i+1)]]\n",
    "        print(input)\n",
    "        input = input+[0]*(n_step-len(input))\n",
    "        print(input)\n",
    "        print('target word: ', words[i+1])\n",
    "        target = word_dict[words[i+1]]\n",
    "        print(target)\n",
    "        \n",
    "        # to check input_batch and target_batch \n",
    "        # uncomment print\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "#         print(input_batch)\n",
    "        target_batch.append(np.eye(n_class)[target])\n",
    "#         print('target array is: ')\n",
    "#         print(target_batch)\n",
    "#         return input_batch, target_batch\n",
    "make_batch_debug(sentences)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make batch input for lstm from sentence data\n",
    "def make_batch(sentence):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    words = word_seq\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        input = [word_dict[n] for n in words[:(i+1)]]\n",
    "        input = input+[0]*(n_step-len(input))\n",
    "        target = word_dict[words[i+1]]\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(np.eye(n_class)[target])\n",
    "        return input_batch, target_batch        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Bi-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 13, 13)\n",
      "(?, 13)\n"
     ]
    }
   ],
   "source": [
    "# creating Bi-LSTM model\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_class])\n",
    "Y = tf.placeholder(tf.float32, [None, n_class])\n",
    "print(X.shape)\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_4:0' shape=(10, 13) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_5:0' shape=(13,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# random weight and bias\n",
    "n_hidden = 5\n",
    "W = tf.Variable(tf.random_normal([n_hidden*2, n_class]))\n",
    "print(W)\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lstm cell\n",
    "lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(n_hidden) # forward lstm\n",
    "lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(n_hidden) # backward lstm\n",
    "\n",
    "# outputs\n",
    "outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, X, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.concat([outputs[0], outputs[1]], 2) # outputs[0]=lstm_fw, outputs[1]=lstm_bw\n",
    "outputs = tf.transpose(outputs, [1, 0, 2]) # [n_step, batch_size, n_hidden]\n",
    "outputs = outputs[-1] # [batch_size, n_hidden]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.matmul(outputs, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.006196\n",
      "Epoch: 2000 cost = 0.001835\n",
      "Epoch: 3000 cost = 0.000830\n",
      "Epoch: 4000 cost = 0.000432\n",
      "Epoch: 5000 cost = 0.000240\n",
      "Epoch: 6000 cost = 0.000137\n",
      "Epoch: 7000 cost = 0.000080\n",
      "Epoch: 8000 cost = 0.000047\n",
      "Epoch: 9000 cost = 0.000028\n",
      "Epoch: 10000 cost = 0.000016\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "input_batch, target_batch = make_batch(sentence)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    _, loss = sess.run([optimizer, cost], feed_dict={X: input_batch, Y: target_batch})\n",
    "    if (epoch + 1)%1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('He is going to Dhaka', 'She lives in Rangpur', 'We love our country')\n",
      "['is']\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "predict = sess.run([prediction], feed_dict={X: input_batch})\n",
    "print(sentences)\n",
    "print([number_dict[n] for n in [pre for pre in predict[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
